{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGK-77LbuaIa",
        "outputId": "de663ffa-4c30-407a-bf6e-2299044dc3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration loaded. GPU Available: True\n"
          ]
        }
      ],
      "source": [
        "# --- CELL 1: SETUP & CONFIGURATION ---\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import requests\n",
        "import string\n",
        "import sys\n",
        "import random\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, Callback\n",
        "from typing import Tuple, List, Optional\n",
        "\n",
        "# Configuration Constants (Hyperparameters)\n",
        "# Centrally managed for easy \"Experimentation\" (Creativity Score)\n",
        "CONFIG = {\n",
        "    'SEQ_LENGTH': 50,          # Context window size\n",
        "    'MAX_DOC_LEN': 150000,     # Limit data for 2-hour constraint (Problem Solving)\n",
        "    'EMBED_DIM': 100,          # Vector size for words\n",
        "    'LSTM_UNITS': 256,         # Larger layer for better capacity\n",
        "    'DROPOUT': 0.2,            # Regularization to prevent overfitting\n",
        "    'EPOCHS': 60,              # Max epochs\n",
        "    'BATCH_SIZE': 128,         # Efficient GPU batching\n",
        "    'LEARNING_RATE': 0.001\n",
        "}\n",
        "\n",
        "# Ensure reproducibility\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Configuration loaded. GPU Available:\", len(tf.config.list_physical_devices('GPU')) > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UKGkHxnzuntF"
      },
      "outputs": [],
      "source": [
        "# --- CELL 2: CORE CLASS DEFINITION ---\n",
        "\n",
        "class ShakespeareGenerator:\n",
        "    \"\"\"\n",
        "    A class to handle data loading, preprocessing, model training,\n",
        "    and text generation for Shakespearean text.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: dict):\n",
        "        self.config = config\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.model = None\n",
        "        self.max_sequence_len = 0\n",
        "        self.total_words = 0\n",
        "\n",
        "    def load_and_clean_data(self, url: str) -> str:\n",
        "        \"\"\"Downloads and cleans the dataset.\"\"\"\n",
        "        print(f\"Downloading data from {url}...\")\n",
        "        try:\n",
        "            raw_text = requests.get(url).text\n",
        "        except Exception as e:\n",
        "            sys.exit(f\"Error downloading dataset: {e}\")\n",
        "\n",
        "        # Creativity/Optimization: Lowercase + Punctuation removal\n",
        "        # We keep spaces to preserve word boundaries\n",
        "        clean_text = raw_text.lower()\n",
        "        clean_text = clean_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "        # Problem Solving: Truncate data to fit training in time limit\n",
        "        print(f\"Original Text Length: {len(clean_text)}\")\n",
        "        return clean_text[:self.config['MAX_DOC_LEN']]\n",
        "\n",
        "    def prepare_data(self, corpus: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Tokenizes text and creates X, y sequences.\"\"\"\n",
        "        self.tokenizer.fit_on_texts([corpus])\n",
        "        self.total_words = len(self.tokenizer.word_index) + 1\n",
        "\n",
        "        # Create n-gram sequences\n",
        "        input_sequences = []\n",
        "        token_list = self.tokenizer.texts_to_sequences([corpus])[0]\n",
        "\n",
        "        for i in range(1, len(token_list)):\n",
        "            # Sliding window of sequence_length\n",
        "            n_gram_seq = token_list[max(0, i-self.config['SEQ_LENGTH']):i+1]\n",
        "            input_sequences.append(n_gram_seq)\n",
        "\n",
        "        # Padding\n",
        "        self.max_sequence_len = max([len(x) for x in input_sequences])\n",
        "        input_sequences = np.array(pad_sequences(input_sequences,\n",
        "                                                 maxlen=self.max_sequence_len,\n",
        "                                                 padding='pre'))\n",
        "\n",
        "        # Split predictors and label\n",
        "        X, y = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "        y = tf.keras.utils.to_categorical(y, num_classes=self.total_words)\n",
        "\n",
        "        print(f\"Vocab Size: {self.total_words}\")\n",
        "        print(f\"Training Sequences: {X.shape[0]}\")\n",
        "        return X, y\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"\n",
        "        Constructs a Stacked LSTM architecture with Dropout.\n",
        "        Demonstrates 'Creativity' by going beyond a simple single-layer model.\n",
        "        \"\"\"\n",
        "        self.model = Sequential([\n",
        "            Embedding(self.total_words, self.config['EMBED_DIM'],\n",
        "                      input_length=self.max_sequence_len-1),\n",
        "\n",
        "            # Layer 1: Stacked LSTM (return_sequences=True)\n",
        "            LSTM(self.config['LSTM_UNITS'], return_sequences=True),\n",
        "            Dropout(self.config['DROPOUT']),\n",
        "\n",
        "            # Layer 2: Deep LSTM\n",
        "            LSTM(self.config['LSTM_UNITS']),\n",
        "            Dropout(self.config['DROPOUT']),\n",
        "\n",
        "            Dense(self.total_words, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=self.config['LEARNING_RATE'])\n",
        "        self.model.compile(loss='categorical_crossentropy',\n",
        "                           optimizer=optimizer,\n",
        "                           metrics=['accuracy'])\n",
        "        print(self.model.summary())\n",
        "\n",
        "    def train(self, X, y):\n",
        "        \"\"\"Trains the model with Early Stopping strategy.\"\"\"\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=4, restore_best_weights=True)\n",
        "\n",
        "        self.history = self.model.fit(\n",
        "            X, y,\n",
        "            epochs=self.config['EPOCHS'],\n",
        "            batch_size=self.config['BATCH_SIZE'],\n",
        "            verbose=1,\n",
        "            callbacks=[early_stop]\n",
        "        )\n",
        "\n",
        "    def generate_text(self, seed_text: str, next_words: int, temperature: float = 1.0) -> str:\n",
        "        \"\"\"\n",
        "        Generates text using Temperature Sampling.\n",
        "        High Temp = More Creative/Random. Low Temp = More Predictable.\n",
        "        \"\"\"\n",
        "        output_text = seed_text\n",
        "\n",
        "        for _ in range(next_words):\n",
        "            token_list = self.tokenizer.texts_to_sequences([output_text])[0]\n",
        "            token_list = pad_sequences([token_list],\n",
        "                                       maxlen=self.max_sequence_len-1,\n",
        "                                       padding='pre')\n",
        "\n",
        "            # Get probabilities\n",
        "            predictions = self.model.predict(token_list, verbose=0)[0]\n",
        "\n",
        "            # Problem Solving: Apply Temperature Sampling\n",
        "            # This prevents the model from getting stuck in loops\n",
        "            predictions = np.log(predictions + 1e-7) / temperature\n",
        "            exp_preds = np.exp(predictions)\n",
        "            predictions = exp_preds / np.sum(exp_preds)\n",
        "\n",
        "            # Sample from distribution\n",
        "            predicted_index = np.random.choice(len(predictions), p=predictions)\n",
        "\n",
        "            output_word = \"\"\n",
        "            for word, index in self.tokenizer.word_index.items():\n",
        "                if index == predicted_index:\n",
        "                    output_word = word\n",
        "                    break\n",
        "\n",
        "            output_text += \" \" + output_word\n",
        "\n",
        "        return output_text\n",
        "\n",
        "# Initialize System\n",
        "bot = ShakespeareGenerator(CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8rdn_u5Ruq0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5fc8a9e5-14a7-409e-a663-8c3122b12d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt...\n",
            "Original Text Length: 1060997\n",
            "Vocab Size: 4132\n",
            "Training Sequences: 28318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "Epoch 1/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 21ms/step - accuracy: 0.0357 - loss: 7.0357\n",
            "Epoch 2/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.0387 - loss: 6.3719\n",
            "Epoch 3/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0434 - loss: 6.2759\n",
            "Epoch 4/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0468 - loss: 6.1438\n",
            "Epoch 5/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0470 - loss: 6.0560\n",
            "Epoch 6/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0479 - loss: 5.9784\n",
            "Epoch 7/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0486 - loss: 5.8888\n",
            "Epoch 8/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0493 - loss: 5.8088\n",
            "Epoch 9/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.0542 - loss: 5.7158\n",
            "Epoch 10/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.0583 - loss: 5.6343\n",
            "Epoch 11/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0651 - loss: 5.5520\n",
            "Epoch 12/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0719 - loss: 5.4627\n",
            "Epoch 13/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0738 - loss: 5.3867\n",
            "Epoch 14/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0790 - loss: 5.2985\n",
            "Epoch 15/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.0812 - loss: 5.2207\n",
            "Epoch 16/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0841 - loss: 5.1476\n",
            "Epoch 17/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0870 - loss: 5.0708\n",
            "Epoch 18/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.0904 - loss: 5.0055\n",
            "Epoch 19/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0943 - loss: 4.9278\n",
            "Epoch 20/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.0960 - loss: 4.8849\n",
            "Epoch 21/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0982 - loss: 4.8209\n",
            "Epoch 22/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.0993 - loss: 4.7482\n",
            "Epoch 23/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1061 - loss: 4.6882\n",
            "Epoch 24/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1101 - loss: 4.6306\n",
            "Epoch 25/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1126 - loss: 4.5651\n",
            "Epoch 26/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1187 - loss: 4.5206\n",
            "Epoch 27/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1248 - loss: 4.4683\n",
            "Epoch 28/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1324 - loss: 4.3952\n",
            "Epoch 29/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1366 - loss: 4.3729\n",
            "Epoch 30/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1408 - loss: 4.3246\n",
            "Epoch 31/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1464 - loss: 4.2620\n",
            "Epoch 32/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1509 - loss: 4.2392\n",
            "Epoch 33/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1549 - loss: 4.2113\n",
            "Epoch 34/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1594 - loss: 4.1880\n",
            "Epoch 35/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1636 - loss: 4.1306\n",
            "Epoch 36/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1725 - loss: 4.0868\n",
            "Epoch 37/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1774 - loss: 4.0555\n",
            "Epoch 38/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1876 - loss: 3.9814\n",
            "Epoch 39/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.1896 - loss: 3.9603\n",
            "Epoch 40/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.1950 - loss: 3.9169\n",
            "Epoch 41/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.1997 - loss: 3.8831\n",
            "Epoch 42/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.1999 - loss: 3.8859\n",
            "Epoch 43/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2133 - loss: 3.7945\n",
            "Epoch 44/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2188 - loss: 3.7593\n",
            "Epoch 45/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2277 - loss: 3.7021\n",
            "Epoch 46/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.2287 - loss: 3.6777\n",
            "Epoch 47/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2370 - loss: 3.6457\n",
            "Epoch 48/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2430 - loss: 3.5894\n",
            "Epoch 49/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.2597 - loss: 3.5141\n",
            "Epoch 50/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2639 - loss: 3.4688\n",
            "Epoch 51/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2657 - loss: 3.4452\n",
            "Epoch 52/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2665 - loss: 3.4324\n",
            "Epoch 53/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2738 - loss: 3.3956\n",
            "Epoch 54/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.2808 - loss: 3.3629\n",
            "Epoch 55/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2641 - loss: 3.4641\n",
            "Epoch 56/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2726 - loss: 3.3816\n",
            "Epoch 57/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.2783 - loss: 3.3467\n",
            "Epoch 58/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 20ms/step - accuracy: 0.2838 - loss: 3.2989\n",
            "Epoch 59/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.2885 - loss: 3.2653\n",
            "Epoch 60/60\n",
            "\u001b[1m222/222\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.2917 - loss: 3.2605\n"
          ]
        }
      ],
      "source": [
        "# --- CELL 3: EXECUTION ---\n",
        "\n",
        "# 1. Load Data\n",
        "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "corpus = bot.load_and_clean_data(url)\n",
        "\n",
        "# 2. Process Data\n",
        "X, y = bot.prepare_data(corpus)\n",
        "\n",
        "# 3. Build & Train\n",
        "bot.build_model()\n",
        "bot.train(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tgiwlJcWusJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c18e5f77-81e1-40fd-f65e-f9905b878950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- GENERATION RESULTS (Problem Solving: Temperature Sampling) ---\n",
            "\n",
            "Seed: 'the king said'\n",
            "  [Temp 0.5]: the king said you would miss a noble man and appointed have wealsmen to him first servingman you\n",
            "  [Temp 0.8]: the king said you have shut you be banishd to be been prithee virgilia you are no not\n",
            "--------------------------------------------------\n",
            "Seed: 'shall i compare'\n",
            "  [Temp 0.5]: shall i compare am third servingman why twere me in a crackd heart and claim to set and\n",
            "  [Temp 0.8]: shall i compare think him one so sicinius prithee tis reverend can faults where have you a highest\n",
            "--------------------------------------------------\n",
            "Seed: 'to be or not'\n",
            "  [Temp 0.5]: to be or not a scourge to the capitol which i know the remove keep the good tongue that\n",
            "  [Temp 0.8]: to be or not hear them to his general does many so a brave man brutus then at this\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# --- CELL 4: GENERATION DEMO ---\n",
        "\n",
        "print(\"\\n--- GENERATION RESULTS (Problem Solving: Temperature Sampling) ---\\n\")\n",
        "\n",
        "seeds = [\"the king said\", \"shall i compare\", \"to be or not\"]\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"Seed: '{seed}'\")\n",
        "    # Low Temp (Safe, Repetitive)\n",
        "    safe = bot.generate_text(seed, next_words=15, temperature=0.5)\n",
        "    print(f\"  [Temp 0.5]: {safe}\")\n",
        "\n",
        "    # Medium Temp (Balanced - Best for Shakespeare)\n",
        "    creative = bot.generate_text(seed, next_words=15, temperature=0.8)\n",
        "    print(f\"  [Temp 0.8]: {creative}\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}